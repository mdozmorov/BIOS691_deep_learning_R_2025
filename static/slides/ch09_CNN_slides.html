<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Introduction to deep learning for computer vision</title>
    <meta charset="utf-8" />
    <meta name="author" content="Mikhail Dozmorov" />
    <meta name="date" content="2025-02-19" />
    <script src="libs/header-attrs-2.29/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="xaringan-my.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Introduction to deep learning for computer vision
]
.subtitle[
## A deep dive
]
.author[
### Mikhail Dozmorov
]
.institute[
### Virginia Commonwealth University
]
.date[
### 2025-02-19
]

---




&lt;!-- .center[&lt;img src="img/.png" height=450&gt;] --&gt;

&lt;!-- .small[  ] --&gt;

## Essential Computer Vision Tasks

.center[&lt;img src="img/computer_vision_overview.png" height=450&gt;]

.small[ https://www.geeksforgeeks.org/computer-vision-tasks/ ]

---
## Image classification

*   **Image classification:**  Assigning one or more labels to an image. Examples: distinguishing cat images from dog images, or tagging images with relevant keywords like "beach," "sunset," and "dog" in Google Photos.
  * **Single-Label Classification:** Each image is assigned to one single category, where the goal is to predict one label per image (e.g., cats vs. dogs)
  * **Multi-Label Classification:** Multiple-Label classification involves assigning multiple labels to an image which has multiple objects.
  
.center[&lt;img src="img/computer_vision_overview.png" height=200&gt;]

.small[ https://luozm.github.io/cv-tasks ]
  
---
## Image segmentation

*   **Image segmentation:** Partitioning an image into distinct areas, each representing a different category. Example: separating a person from the background in a video call to apply a virtual background.
  * **Semantic Segmentation:** Assigning a class label to each individual pixel in an image. The output is a 'segmentation map' where each pixel's color represents its class.
  * **Instance Segmentation:** Identifying and delineating each individual instance of those objects.

.center[&lt;img src="img/computer_vision_overview.png" height=200&gt;]

.small[ https://luozm.github.io/cv-tasks ]

---  
## Object detection

*   **Object detection:** Drawing bounding boxes around objects in an image and labeling each box with a class. Example: a self-driving car identifying pedestrians, cars, and traffic signs.
  * **Object Localization:** Use bounding box to mark the object locations in an image or tracking a moving object in a video.
  * **Object Classification:** Putting each object into a pre-defined category like 'human', 'car', or 'animal'.

.center[&lt;img src="img/computer_vision_overview.png" height=250&gt;]

.small[ https://luozm.github.io/cv-tasks ]

---
## Modern convnet architecture patterns

*   **Modularity, Hierarchy, and Reuse (MHR):**  This fundamental principle involves structuring a system into reusable modules arranged in a hierarchy. In the context of convnets, this translates to building models with blocks of layers, repeated groups of layers, and pyramid-like structures with increasing filter counts and decreasing feature map sizes.

.center[&lt;img src="img/vgg16.jpg" height=350&gt;]

.small[ https://www.geeksforgeeks.org/vgg-16-cnn-model/ ]

---
## The convolutional layer
.center[&lt;img src="img/cnn_overview.png" height=250&gt;]

.center[&lt;img src="img/convolution_operation.png" height=250&gt;]
.small[https://www.superannotate.com/blog/guide-to-convolutional-neural-networks]

---
## Residual connections

*   **Residual Connections:** This technique addresses the **vanishing gradient problem** encountered in deep networks. By adding the input of a layer or block back to its output, information can bypass potentially destructive or noisy transformations, ensuring gradient information from earlier layers propagates effectively. This allows for the training of much deeper networks.

.center[&lt;img src="img/residual_connections.png" height=350&gt;]

.small[ https://cdanielaam.medium.com/understanding-residual-connections-in-neural-networks-866b94f13a22 ]

---
## Batch normalization

*   **Batch Normalization:** This technique normalizes intermediate activations during training, helping with gradient propagation and potentially enabling deeper networks. It uses the mean and variance of the current batch to normalize data during training and relies on an exponential moving average of these statistics during inference.

.center[&lt;img src="img/batch_normalization.png" height=350&gt;]

.small[ https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739 ]

---
## Depthwise Separable Convolutions

*   **Depthwise Separable Convolutions:** This approach separates the learning of spatial and channel-wise features, reducing the number of parameters and computations compared to regular convolutions. This results in smaller models that converge faster, generalize better, and are less prone to overfitting. 
  *   **Xception**, a high-performing convnet architecture available in Keras, utilizes depthwise separable convolutions extensively.

.center[&lt;img src="img/xception.png" height=250&gt;]
.small[ https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568 ]

---
## Techniques for Interpreting ConvNet Decisions

While deep learning models are often seen as "black boxes," convnets offer several techniques for visualizing and understanding their decisions.  **Three key methods** for interpreting what convnets learn:

- **Visualizing intermediate convnet outputs (intermediate activations)**
- **Visualizing convnet filters**
- **Visualizing heatmaps of class activation in an image**

---
## Visualizing intermediate convnet outputs (intermediate activations)

*   Displaying how the input is transformed by successive layers and gain insights into the meaning of individual filters within the network.

*   As you go deeper into the network, activations become more abstract and less visually interpretable. They transition from encoding low-level features like edges to higher-level concepts like "cat ear" or "cat eye." This signifies a shift towards more specific and abstract representations.
    
---
## Visualizing intermediate convnet outputs (intermediate activations)

.center[&lt;img src="img/conv2d_1.png" height=450&gt;]

.small[ https://www.geeksforgeeks.org/visualizing-representations-of-outputs-activations-of-each-cnn-layer/ ]

---
## Visualizing convnet filters

*   This technique aims to understand the specific visual patterns or concepts that each filter in a convnet is designed to detect.

*   It leverages **gradient ascent in input space** to generate an image that maximally activates a chosen filter. Starting with a blank input image, the process adjusts the image's pixel values to maximize the filter's response.

*   Visualizations of filters at different depths of the Xception model reveal a hierarchical learning process, with early layers detecting basic features like edges and colors, while deeper layers recognize complex textures and object parts.

---
## Visualizing convnet filters

.center[&lt;img src="img/convnet_filters.png" height=450&gt;]

.small[ https://stackoverflow.com/questions/38450104/how-to-visualize-filters-after-the-1st-layer-trained-by-cnns ]

---
## Visualizing heatmaps of class activation in an image

*   This technique helps understand which parts of an image contributed most significantly to the model's classification decision.

*   This method is particularly useful for debugging misclassifications and gaining insights into the model's decision-making process. 

*   **Class activation map (CAM) visualization**, **Grad-CAM**, weighs each channel in the output feature map of a convolution layer by the gradient of the target class with respect to that channel. This effectively highlights the regions in the image that are most strongly associated with the chosen class.

---
## Visualizing heatmaps of class activation in an image

.center[&lt;img src="img/keras_gradcam_header.jpg" height=450&gt;]

.small[ https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/ ]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
