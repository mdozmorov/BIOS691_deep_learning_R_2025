<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>The Universal Workflow of Machine Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Mikhail Dozmorov" />
    <meta name="date" content="2025-02-05" />
    <script src="libs/header-attrs-2.29/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="xaringan-my.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# The Universal Workflow of Machine Learning
]
.author[
### Mikhail Dozmorov
]
.institute[
### Virginia Commonwealth University
]
.date[
### 2025-02-05
]

---




&lt;!-- .center[&lt;img src="img/.png" height=450&gt;] --&gt;

&lt;!-- .small[  ] --&gt;


## Starting with Deep Learning

* **Real-World Machine Learning Projects** originate from a problem or a business need. E.g., personalized photo search engines, spam detection, music recommendation systems, credit card fraud detection, and more.

## Defining the Task: Understanding the Context

* **Engaging with Stakeholders.** Thoroughly understand the project's context and the expectations. This involves:
    * **Understanding Business Logic:**  Determining why the customer needs a solution, how the model will be used, its value proposition, and how it integrates into existing business processes.
    * **Data Exploration:** Identifying available data sources and potential data collection strategies.
    * **Task Mapping:** Determining the appropriate machine learning task to address the business problem â€“ classification, regression, ranking, etc..

---
## Framing the Problem

* **Input and Output Data:**  Clearly define the input data (features) and the desired output (target variable). Emphasize that data availability often dictates what can be predicted. Supervised learning requires labeled data for both inputs and targets. 

* **Machine Learning Task Type:**  Accurately categorize the problem. Is it binary classification, multiclass classification, regression, ranking, or something else? Examples from the sources can be used to illustrate this (spam detection, music recommendation, etc.).

---
## Framing the Problem

* **Existing Solutions:**  Investigate any existing solutions currently used by the customer.  These might be rule-based systems or manual processes. Understanding them helps in setting baselines and understanding potential improvements.

* **Constraints:**  Identify any limitations in the project. For example, data privacy concerns might necessitate on-device processing, or real-time applications might demand strict latency limits.

* **Formulating Hypotheses:**  Clearly state the assumptions being made. Can the target be predicted from the inputs? Is the available data sufficient and informative enough?  These hypotheses are validated or refuted during model development.

---
## Collecting a Dataset

* Data is the foundation of machine learning. The quality and quantity of data heavily influence a model's ability to generalize.

* **Data Collection Strategies:** Manual Annotation (e.g., tagging images or labeling text) and/or Automatic Retrieval (e.g., using user "likes" for music recommendation).

* **Data Representativeness:** The training data should closely mirror the real-world data the model will encounter (avoid sampling bias). The nature of data can change over time (concept drift), necessitating model retraining and updates. 

---
## Understanding Your Data

* **Exploratory Data Analysis:**  Visualizing data can reveal valuable insights and potential issues.
    * **Data Imbalance:**  Identify and handle class imbalances in classification problems.
    * **Missing Values:**  Use techniques for dealing with missing values, such as creating a new category, using mean/median imputation, or even training a model to predict missing values.
    * **Target Leaking:**  Warn against the inclusion of features that leak information about the target variable, as this leads to artificially inflated performance.

---
## Choosing a Measure of Success

* **Metric Selection:** Factors to consider include the nature of the problem (balanced vs. imbalanced classification), the type of task (classification or regression), and the alignment with business goals. The metric for success will guide all of the technical choices you make throughout the project.

---
## Choosing a Measure of Success

- **Accuracy** (the fraction of predictions a model gets right) is a common metric for balanced classification where every class is equally likely.

- **ROC AUC** (the area under the receiver operating characteristic curve) is another common metric for balanced classification problems.

- **Precision** (What proportion of positive identifications was actually correct?) and **Recall** (What proportion of actual positives was identified correctly?) used for Class-imbalanced problems, Ranking problems, Multilabel classification.

- **Categorical crossentropy** and **Binary crossenthropy** measure the distance between two probability distributions.

- **Mean squared error (MSE)** and **Mean absolute error (MAE) ** are common metrics for regression problems.

---
## Developing a Model

- Research existing solutions, feature engineering techniques, and architectures used for similar problems.

- Prepare the Data, convert various data types (text, images, sound) into numerical tensors that neural networks can process.
    * **Normalization:** normalize features to avoid issues with gradient updates and to aid in model convergence.
    * **Handling Missing Values:** new category, replace with average, impute/predict.

---
## Choosing an Evaluation Protocol

* **Reliable Performance Estimation:**  Use unseen data (validation dataset) to accurately estimate model's performance.
    * **Holdout Validation Set:**  When sufficient data is available, keep a part of it for validation.
    * **K-fold Cross-Validation:**  Describe its benefits when data is limited and how it provides a more robust performance estimate by averaging across multiple folds. Refer back to the previous discussion and example on K-fold cross-validation.

---
## Compare with a Baseline

- A baseline model is a simple model that is used to measure the performance of a more complex model. The baseline model should be easy to implement and should not require a lot of tuning.
  - You should select a simple baseline to beat during the model development process.
  - Your initial goal should be to achieve statistical power, which means developing a small model that is capable of beating a simple baseline.

---
## Compare with a Baseline

- If you cannot beat a simple baseline after trying multiple reasonable architectures, the answer to the question you're asking may not be present in the input data.
  * **Feature Engineering:**  Selecting informative features and creating new ones based on domain knowledge.
  * **Architecture Selection:**  Choosing the appropriate model architecture (densely connected networks, CNNs, RNNs, etc.) for the task.
  * **Training Configuration:**  Selecting an appropriate loss function, batch size, and learning rate. 

---
## Scale Up: Overfit, Regularize and Tune Your Model

* Push the model to overfit in order to determine its capacity and find the optimal balance between underfitting and overfitting.

* Maximize Generalization, the model's performance on unseen data.
  * **Regularization Techniques:** Dropout (randomly dropping units during training) and/or Weight Regularization (L1 and L2 regularization).
  * **Hyperparameter Tuning:** Find the optimal model configuration (number of units, learning rate, etc.). KerasTuner can automate this.

* Avoid Information Leakage from validation data, as it can lead to overfitting dto the validation process itself and make evaluation less reliable.

---
## Deploying the Model: From Training to Production

- Explain Your Work to Stakeholders and Set Expectations
  * Set realistic expectations about the model's capabilities and limitations.
  * Explain that AI systems don't possess human-like understanding or common sense.
  * Use metrics relevant to business goals (false positive rates, false negative rates) instead of abstract accuracy numbers.

* Decide on model deployment method: REST API, On-device or browser deployment

* Monitor the model to track performance, behavior, and business impact.

* Maintain Your Model's performance and adjust given new data and/or concept drift.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
