---
title: "BIOS691, Spring 2025"
output:
  word_document: default
  html_document: default
# date: "2024-09-17"
---



<div id="course-title-deep-learning-with-r" class="section level1">
<h1>Course Title: Deep Learning with R</h1>
<p>Instructor: Mikhail Dozmorov<br />
Department: Biostatistics, VCU<br />
Credits: 3<br />
Duration: 15 weeks (2 lectures per week, 1 hour 20 minutes each)</p>
<div id="course-overview" class="section level2">
<h2>Course Overview</h2>
<!-- This course introduces deep learning concepts using R, with hands-on exercises. The focus will be on implementing deep learning models and exploring various architectures using the "Deep Learning with R" textbook by François Chollet and J. J. Allaire. The course will cover both foundational and advanced topics in deep learning, including computer vision, natural language processing, and generative models. -->
<p>Deep learning is an actively growing machine learning field for many research and application areas, such as computer vision, speech recognition, time series forecasting. It is becoming the state-of-the-art approach among machine learning methods, especially suitable for extracting useful information from large, unstructured datasets.</p>
<p>This course is an introduction to deep learning theory and practice. It will cover the basics of neural network architectures, main statistical concepts behind training neural networks, and implementation aspects. The main focus will be on programming deep neural networks using TensorFlow and its Keras front-end in R, although the knowledge will also be useful for Python practitioners. The goal of this course is to build a foundation for general understanding of deep learning and hands-on implementation of main types of neural network architectures, and provide material for further development.</p>
<p>The class will be conducted in person and include lecture and coding parts. Course material will be publicly available. The syllabus is subject to change. Observe the <a href="https://students.vcu.edu/studentconduct/vcu-honor-system/academic-misconduct-/honor-pledge/">VCU Honor Pledge</a> in any class- and homework activities.</p>
</div>
<div id="prerequisites" class="section level2">
<h2>Prerequisites</h2>
<ul>
<li>Book
<ul>
<li><a href="https://www.manning.com/books/deep-learning-with-r-second-edition"><strong>Deep learning with R</strong></a> by François Chollet (the creator of Keras) with J. J. Allaire (the founder of RStudio and the author of the R interfaces to Keras and TensorFlow)</li>
</ul></li>
</ul>
<!--    - [The Deep Learning textbook](https://www.deeplearningbook.org/) by Ian Goodfellow, Yoshua Bengio and Aaron Courville
    - [MIT Introduction to Deep Learning | 6.S191](https://www.youtube.com/watch?v=njKP3FqW3Sk&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) - MIT video course by Alexander Amini, Ava Soleimani, and guests. Dense and informative ~45min lectures covering various topics of deep learning. [introtodeeplearning.com](http://introtodeeplearning.com/) - course web site with slides, video, and other material
    - [Machine learning and deep learning resources](https://github.com/mdozmorov/MachineLearning_notes) - a collection of references for further studies
- Code
    - [**R notebooks for the code samples of the book "Deep Learning with R"**](https://github.com/jjallaire/deep-learning-with-r-notebooks) and the [second edition code](https://github.com/t-kalinowski/deep-learning-with-R-2nd-edition-code)
    - [Deep Learning with Keras and TensorFlow in R Workflow](https://github.com/rstudio-conf-2020/dl-keras-tf) - RStudio Conference 2020 workshop by Brad Boehmke
-->
<ul>
<li>Skills
<ul>
<li>Working knowledge of R, familiarity with RStudio programming environment, command line, GitHub (BIOS524)</li>
<li>Basic linear algebra: vectors, matrices, determinants</li>
<li>Simple calculus: derivatives, integrals, gradients</li>
<li>Some probability theory: probability, random variables, distributions</li>
<li>Basic statistics knowledge: descriptive statistics, estimators.</li>
<li>(Linear) modeling</li>
</ul></li>
<li>Hardware
<ul>
<li>A laptop, Mac or Linux OSs are recommended. GPU (graphics processing unit) is not required</li>
</ul></li>
<li>Software
<ul>
<li>R for <a href="http://cran.r-project.org/bin/windows/base/"><strong>Windows</strong></a> or <a href="http://cran.r-project.org/bin/macosx/"><strong>Mac</strong></a>. Review <a href="https://ismayc.github.io/rbasics-book/"><strong>Getting Used to R, RStudio, and RMarkdown</strong></a> book, if necessary</li>
<li><a href="https://www.rstudio.com/products/rstudio/download/"><strong>RStudio Desktop</strong></a></li>
</ul></li>
</ul>
<!--    - [**Git**](https://git-scm.com/downloads)
    - A text editor ([**Notepad++**](https://notepad-plus-plus.org/) on Windows, or [**Sublime text**](https://www.sublimetext.com/) on any platform)
    - Optional: Docker for [**Windows**](https://hub.docker.com/editions/community/docker-ce-desktop-windows/) or [**Mac**](https://hub.docker.com/editions/community/docker-ce-desktop-mac/) is recommended
    - Windows only: [**Git Bash**](https://git-for-windows.github.io/) or [**Cygwin**](http://www.cygwin.com/)
    - Windows only: [**Rtools**](https://cran.r-project.org/bin/windows/Rtools/)
    - [**Windows-specific instructions on installing Keras and TensorFlow**](tensorflow.html)
-->
<div id="course-objectives" class="section level3">
<h3>Course Objectives</h3>
<ul>
<li>Understand the fundamental concepts of deep learning and its applications</li>
<li>Implement deep learning models in R using Keras and TensorFlow</li>
<li>Explore key architectures like convolutional and recurrent neural networks</li>
<li>Apply deep learning techniques to text, sequences, and images</li>
<li>Work with generative models such as variational autoencoders and GANs</li>
</ul>
</div>
<div id="tentative-schedule" class="section level3">
<h3>Tentative schedule</h3>
<div id="week-1-introduction-to-deep-learning" class="section level4">
<h4>Week 1: Introduction to Deep Learning</h4>
<!-- - Lecture 1: What is Deep Learning? (Ch. 1) -->
<!-- AI, machine learning, deep learning concepts -->
<!-- Achievements and current limitations -->
<!-- - Lecture 2: Brief History of Machine Learning (Ch. 1.2) -->
<!-- Evolution of neural networks and the ML landscape -->
</div>
<div id="week-2-mathematical-foundations" class="section level4">
<h4>Week 2: Mathematical Foundations</h4>
<!-- - Lecture 3: First Look at Neural Networks (Ch. 2.1) -->
<!-- /Users/mdozmorov/Documents/Data/MachineLearning/deep-learning-with-r-notebooks/notebooks/2.1-a-first-look-at-a-neural-network.Rmd -->
<!-- - Lecture 4: Data Representations for Neural Networks (Ch. 2.2) -->
<!-- Tensors, data structures, and real-world examples -->
</div>
<div id="week-3-neural-network-mechanics" class="section level4">
<h4>Week 3: Neural Network Mechanics</h4>
<!-- - Lecture 5: Tensor Operations (Ch. 2.3) -->
<!-- Element-wise operations, tensor reshaping -->
<!-- - Lecture 6: Gradient-Based Optimization (Ch. 2.4) -->
<!-- Backpropagation and stochastic gradient descent -->
</div>
<div id="week-4-neural-networks-in-practice" class="section level4">
<h4>Week 4: Neural Networks in Practice</h4>
<!-- - Lecture 7: Introduction to Keras (Ch. 3.2) -->
<!-- Setting up a deep learning workstation -->
<!-- - Lecture 8: Binary Classification (Ch. 3.4) -->
<!-- Classifying movie reviews with neural networks -->
</div>
<div id="week-5-advanced-classification-and-regression" class="section level4">
<h4>Week 5: Advanced Classification and Regression</h4>
<!-- - Lecture 9: Multiclass Classification (Ch. 3.5) -->
<!-- Classifying newswires with neural networks -->
<!-- - Lecture 10: Regression with Neural Networks (Ch. 3.6) -->
<!-- Predicting house prices with K-fold validation -->
</div>
<div id="week-6-machine-learning-fundamentals" class="section level4">
<h4>Week 6: Machine Learning Fundamentals</h4>
<!-- - Lecture 11: Overview of Machine Learning (Ch. 4.1) -->
<!-- Supervised, unsupervised, reinforcement learning -->
<!-- - Lecture 12: Evaluating Machine-Learning Models (Ch. 4.2) -->
<!-- Train/test splits, validation strategies -->
</div>
<div id="week-7-data-preprocessing-overfitting" class="section level4">
<h4>Week 7: Data Preprocessing &amp; Overfitting</h4>
<!-- - Lecture 13: Preprocessing and Feature Engineering (Ch. 4.3) -->
<!-- Preparing data for neural networks -->
<!-- - Lecture 14: Overfitting and Underfitting (Ch. 4.4) -->
<!-- Regularization techniques: dropout, weight decay -->
</div>
<div id="week-8-convolutional-neural-networks-cnns" class="section level4">
<h4>Week 8: Convolutional Neural Networks (CNNs)</h4>
<!-- - Lecture 15: Introduction to Convnets (Ch. 5.1) -->
<!-- Convolution and max-pooling operations -->
<!-- - Lecture 16: Training CNNs from Scratch (Ch. 5.2) -->
<!-- Hands-on with a small dataset -->
</div>
<div id="week-9-transfer-learning-and-visualization" class="section level4">
<h4>Week 9: Transfer Learning and Visualization</h4>
<!-- - Lecture 17: Using Pretrained Convnets (Ch. 5.3) -->
<!-- Fine-tuning and feature extraction -->
<!-- - Lecture 18: Visualizing Convnets (Ch. 5.4) -->
<!-- Filter visualization, heatmaps -->
</div>
<div id="week-10-text-and-sequence-data" class="section level4">
<h4>Week 10: Text and Sequence Data</h4>
<!-- - Lecture 19: Working with Text Data (Ch. 6.1) -->
<!-- Embeddings, one-hot encoding -->
<!-- - Lecture 20: Recurrent Neural Networks (RNNs) (Ch. 6.2) -->
<!-- LSTM, GRU layers -->
</div>
<div id="week-11-advanced-rnns-and-sequence-processing" class="section level4">
<h4>Week 11: Advanced RNNs and Sequence Processing</h4>
<!-- - Lecture 21: Advanced Use of RNNs (Ch. 6.3) -->
<!-- Stacking layers, recurrent dropout -->
<!-- - Lecture 22: Sequence Processing with Convnets (Ch. 6.4) -->
<!-- Combining CNNs and RNNs -->
</div>
<div id="week-12-advanced-architectures" class="section level4">
<h4>Week 12: Advanced Architectures</h4>
<!-- - Lecture 23: The Functional API in Keras (Ch. 7.1) -->
<!-- Multi-input/output models, layer sharing -->
<!-- - Lecture 24: Model Inspection and Monitoring (Ch. 7.2) -->
<!-- Keras callbacks and TensorBoard -->
</div>
<div id="week-13-best-practices-and-hyperparameter-tuning" class="section level4">
<h4>Week 13: Best Practices and Hyperparameter Tuning</h4>
<!-- - Lecture 25: Advanced Architecture Patterns (Ch. 7.3) -->
<!-- Hyperparameter optimization and ensembling -->
<!-- - Lecture 26: Model Ensembling and Best Practices (Ch. 7.3) -->
</div>
<div id="week-14-generative-models" class="section level4">
<h4>Week 14: Generative Models</h4>
<!-- - Lecture 27: Text Generation with LSTM (Ch. 8.1) -->
<!-- Character-level LSTM, sampling strategies -->
<!-- - Lecture 28: Neural Style Transfer (Ch. 8.3) -->
<!-- Implementing style transfer in Keras -->
</div>
<div id="week-15-variational-autoencoders-and-gans" class="section level4">
<h4>Week 15: Variational Autoencoders and GANs</h4>
<!-- - Lecture 29: Variational Autoencoders (Ch. 8.4) -->
<!-- Concept vectors, latent spaces -->
<!-- - Lecture 30: Generative Adversarial Networks (Ch. 8.5) -->
<!-- DCGAN and adversarial training -->
</div>
</div>
</div>
<div id="grading" class="section level2">
<h2>Grading</h2>
<ul>
<li>Assignments (60%): Regular homework assignments based on the book and additional datasets</li>
<li>Participation (40%): Attendance and active participation in class discussions and labs</li>
</ul>
<!--
# Chapters

- Chapter 1: What is deep learning?
- Chapter 2: The mathematical building blocks of neural networks
- Chapter 3: Introduction to Keras and TensorFlow
- Chapter 4: Getting started with neural networks: Classification and regression
- Chapter 5: Fundamentals of machine learning
- Chapter 6: The universal workflow of machine learning
- Chapter 7: Working with Keras: A deep dive
- Chapter 8: Introduction to deep learning for computer vision
- Chapter 9: Advanced deep learning for computer vision
- Chapter 10: Deep learning for time series
- Chapter 11: Deep learning for text
- Chapter 12: Generative deep learning
- Chapter 13: Best practices and advanced techniques
- Chapter 14: Conclusions
- Appendix: Python primer for R users



# Lecture Schedule: Deep Learning with R

## Unit 1: Foundations of Deep Learning (Lectures 1–7)

### Lecture 1:
Overview: What is Deep Learning?
AI, Machine Learning, and Deep Learning
History of Machine Learning
What Makes Deep Learning Unique

### Lecture 2:
Why Deep Learning?
Hardware, Data, and Algorithms
Investment Trends and Democratization
Limitations and Future Promise

### Lecture 3:
Mathematical Building Blocks of Neural Networks: Scalars, Vectors, and Tensors
Data Representations (Rank 0-3 Tensors)
Real-World Examples (Vector, Time-Series, Image, Video Data)
Lecture 4:

Tensor Operations and Their Geometric Interpretation
Broadcasting, Reshaping, Tensor Products
Lecture 5:

Neural Network Optimization: Gradient Descent and Backpropagation
Derivatives and Gradients
Stochastic Gradient Descent
Lecture 6:

TensorFlow and Keras: Introduction and Installation
Setting Up the Workspace
First Steps with TensorFlow Tensors
Lecture 7:

Anatomy of Neural Networks in Keras
Layers, Models, and the Compile Step
Monitoring Metrics and Using Trained Models
Unit 2: Core Applications of Deep Learning (Lectures 8–15)
Lecture 8:

Binary Classification with Neural Networks
IMDB Dataset: Preparing Data and Building a Model
Lecture 9:

Multiclass Classification
Reuters Dataset: Handling Multiple Classes
Lecture 10:

Regression with Neural Networks
Boston Housing Prices: K-Fold Validation
Lecture 11:

Generalization and Overfitting
Training, Validation, and Test Sets
Evaluating Machine Learning Models
Lecture 12:

Improving Model Fit
Hyperparameter Tuning
Model Regularization Techniques
Lecture 13:

Universal Workflow of Machine Learning (Part 1)
Framing Problems and Dataset Curation
Lecture 14:

Universal Workflow of Machine Learning (Part 2)
Model Development and Deployment
Lecture 15:

Writing Custom Training Loops and Callbacks in Keras
Unit 3: Advanced Architectures and Techniques (Lectures 16–24)
Lecture 16:

Introduction to Convolutional Neural Networks
The Convolution and Max-Pooling Operations
Lecture 17:

Training a CNN from Scratch
Data Augmentation and Small Dataset Training
Lecture 18:

Leveraging Pretrained Models
Feature Extraction and Fine-Tuning
Lecture 19:

Modern CNN Architectures
Residual Connections and Batch Normalization
Lecture 20:

Introduction to Recurrent Neural Networks (RNNs)
Temperature Forecasting Example
Lecture 21:

Advanced RNN Techniques
Bidirectional Layers and Recurrent Dropout
Lecture 22:

Deep Learning for Text Data
Preparing Text Data and Tokenization
Lecture 23:

The Transformer Architecture
Understanding Self-Attention
Lecture 24:

Sequence-to-Sequence Models
Machine Translation Example
Unit 4: Generative Models and Real-World Applications (Lectures 25–30)
Lecture 25:

Introduction to Generative Deep Learning
Text Generation with RNNs
Lecture 26:

Neural Style Transfer
Implementing Style and Content Loss
Lecture 27:

Variational Autoencoders (VAEs)
Concept Vectors and Latent Space Sampling
Lecture 28:

Generative Adversarial Networks (GANs)
Implementing a Basic GAN
Lecture 29:

Best Practices for Model Training and Optimization
Hyperparameter Search and Mixed Precision
Lecture 30:

Deep Learning Frontiers
Lifelong Learning, Modular Architectures, and Staying Updated




## Table of content

1 What is deep learning?
1.1 Artificial intelligence, machine learning, and deep learning
Artificial intelligence
Machine learning
Learning rules and representations from data
The “deep” in “deep learning”
Understanding how deep learning works, in three figures
What deep learning has achieved so far
Don’t believe the short-term hype
The promise of AI

1.2 Before deep learning: A brief history of machine learning Probabilistic modeling
Early neural networks
Kernel methods
Decision trees, random forests, and gradient-boosting machines
Back to neural networks
What makes deep learning different?
The modern machine learning landscape

1.3 Why deep learning? Why now?
Hardware
Data
Algorithms
A new wave of investment
The democratization of deep learning
Will it last?

2 The mathematical building blocks of neural networks
2.1 A first look at a neural network
2.2 Data representations for neural networks
Scalars (rank 0 tensors)
Vectors (rank 1 tensors)
Matrices (rank 2 tensors)
Rank 3 and higher-rank tensors
Key attributes
Manipulating tensors in R
The notion of data batches
Real-world examples of data tensors
Vector data
Time-series data or sequence data
Image data
Video data
2.3 The gears of neural networks: Tensor operations
Element-wise operations
Broadcasting
Tensor product
Tensor reshaping
Geometric interpretation of tensor operations
A geometric interpretation of deep learning
2.4 The engine of neural networks: Gradient-based optimization
What’s a derivative?
Derivative of a tensor operation: The gradient
Stochastic gradient descent
Chaining derivatives: The backpropagation algorithm
2.5 Looking back at our first example
Reimplementing our first example from scratch in TensorFlow
Running one training step
The full training loop
Evaluating the model

3 Introduction to Keras and TensorFlow
3.1 What’s TensorFlow?
3.2 What’s Keras?
3.3 Keras and TensorFlow: A brief history
3.4 Python and R interfaces: A brief history
3.5 Setting up a deep learning workspace
Installing Keras and TensorFlow
3.6 First steps with TensorFlow
TensorFlow tensors
3.7 Tensor attributes
Tensor shape and reshaping
Tensor slicing
Tensor broadcasting
The tf module
Constant tensors and variables
Tensor operations: Doing math in TensorFlow
A second look at the GradientTape API
An end-to-end example: A linear classifier in pure TensorFlow
3.8 Anatomy of a neural network: Understanding core Keras APIs
Layers: The building blocks of deep learning
From layers to models
The “compile” step: Configuring the learning process
Picking a loss function
Understanding the fit() method
Monitoring loss and metrics on validation data
Inference: Using a model after training

4 Getting started with neural networks: Classification and regression
4.1 Classifying movie reviews: A binary classification example
The IMDB dataset
Preparing the data
Building your model
Validating your approach
Using a trained model to generate predictions on new data
Further experiments
Wrapping up
4.2 Classifying newswires: A multiclass classification example
The Reuters dataset
Preparing the data
Building your model
Validating your approach
Generating predictions on new data
A different way to handle the labels and the loss
The importance of having sufficiently large intermediate layers
Further experiments
Wrapping up
4.3 Predicting house prices: A regression example
The Boston housing price dataset
Preparing the data
Building your model
Validating your approach using K-fold
validation
Generating predictions on new data
Wrapping up

5 Fundamentals of machine learning
5.1 Generalization: The goal of machine learning
Underfitting and overfitting
The nature of generalization in deep learning
5.2 Evaluating machine learning models
Training, validation, and test sets
Beating a common-sense baseline
Things to keep in mind about model evaluation
5.3 Improving model fit
Tuning key gradient descent parameters
Leveraging better architecture priors
Increasing model capacity
5.4 Improving generalization
Dataset curation
Feature engineering
Using early stopping
Regularizing your model

6 The universal workflow of machine learning
6.1 Define the task
Frame the problem
Collect a dataset
Understand your data
Choose a measure of success
6.2 Develop a model
Prepare the data
Choose an evaluation protocol
Beat a baseline
Scale up: Develop a model that overfits
Regularize and tune your model
6.3 Deploy the model
Explain your work to stakeholders and set expectations
Ship an inference model
Monitor your model in the wild
Maintain your model

7 Working with Keras: A deep dive
7.1 A spectrum of workflows
7.2 Different ways to build Keras models
The Sequential model
The Functional API
Subclassing the Model class
Mixing and matching different components
Remember: Use the right tool for the job
7.3 Using built-in training and evaluation loops
Writing your own metrics
Using callbacks
Writing your own callbacks
Monitoring and visualization with
TensorBoard
7.4 Writing your own training and evaluation loops
Training vs. inference
Low-level usage of metrics
A complete training and evaluation loop
Make it fast with tf_function()
Leveraging fit() with a custom training loop

8 Introduction to deep learning for computer vision
8.1 Introduction to convnets
The convolution operation
The max-pooling operation
8.2 Training a convnet from scratch on a small dataset
The relevance of deep learning for small data problems
Downloading the data
Building the model
Data preprocessing
Using data augmentation
8.3 Leveraging a pretrained model
Feature extraction with a pretrained model
Fine-tuning a pretrained model

9 Advanced deep learning for computer vision
9.1 Three essential computer vision tasks
9.2 An image segmentation example
9.3 Modern convnet architecture patterns
Modularity, hierarchy, and reuse
Residual connections
Batch normalization
Depthwise separable convolutions
Putting it together: A mini Xception-like model
9.4 Interpreting what convnets learn
Visualizing intermediate activations
Visualizing convnet filters
Visualizing heatmaps of class activation

10 Deep learning for time series
10.1 Different kinds of time-series tasks
10.2 A temperature-forecasting example
Preparing the data
A common-sense, non–machine learning baseline
Let’s try a basic machine learning model
Let’s try a 1D convolutional model
A first recurrent baseline
10.3 Understanding recurrent neural networks
A recurrent layer in Keras
10.4 Advanced use of recurrent neural networks
Using recurrent dropout to fight overfitting
Stacking recurrent layers
Using bidirectional RNNs
Going even further

11 Deep learning for text
11.1 Natural language processing: The bird’s-eye view
11.2 Preparing text data
Text standardization
Text splitting (tokenization)
Vocabulary indexing
Using layer_text_vectorization
11.3 Two approaches for representing groups of words:
Sets and sequences
Preparing the IMDB movie reviews data
Processing words as a set: The bag-of-words approach
Processing words as a sequence: The sequence model approach
11.4 The Transformer architecture
Understanding self-attention
Multi-head attention
The Transformer encoder
When to use sequence models over bag-of-words models
11.5 Beyond text classification: Sequence-to-sequence learning
A machine translation example
Sequence-to-sequence
learning with RNNs
Sequence-to-sequence learning with
Transformer

12 Generative deep learning
12.1 Text generation
A brief history of generative deep learning for sequence generation
How do you generate sequence data?
The importance of the sampling strategy
Implementing text generation with Keras
A text-generation callback with variable-temperature sampling
Wrapping up
12.2 DeepDream
Implementing DeepDream in Keras
Wrapping up
12.3 Neural style transfer
The content loss
The style loss
Neural style transfer in Keras
Wrapping up
12.4 Generating images with variational autoencoders
Sampling from latent spaces of images
Concept vectors for image editing
Variational autoencoders
Implementing a VAE with Keras
Wrapping up
12.5 Introduction to generative adversarial networks
A schematic GAN implementation
A bag of tricks
Getting our hands on the CelebA dataset
The discriminator
The generator
The adversarial network
Wrapping up

13 Best practices for the real world
13.1 Getting the most out of your models
Hyperparameter optimization
Model ensembling
13.2 Scaling-up model training
Speeding up training on GPU with mixed precision Multi-GPU training
TPU training

14 Conclusions
14.1 Key concepts in review
Various approaches to AI
What makes deep learning special within the field of machine learning
How to think about deep learning
Key enabling technologies
The universal machine learning workflow
Key network architectures
The space of possibilities
14.2 The limitations of deep learning
The risk of anthropomorphizing machine learning models
Automatons vs. intelligent agents
Local generalization vs. extreme generalization
The purpose of intelligence
Climbing the spectrum of generalization
14.3 Setting the course toward greater generality in AI
On the importance of setting the right objective: The shortcut rule
A new target
14.4 Implementing intelligence: The missing ingredients
Intelligence as sensitivity to abstract analogies
The two poles of abstraction
The two poles of abstraction
The missing half of the picture
14.5 The future of deep learning
Models as programs
Machine learning vs. program synthesis
Blending together deep learning and program synthesis
Lifelong learning and modular subroutine reuse
The long-term vision
14.6 Staying up-to-date in a fast-moving field
Practice on real-world problems using Kaggle
Read about the latest developments on arXiv
Explore the Keras ecosystem
14.7 Final words
-->
</div>
</div>
